# Microservicio Orquestador LLM

Microservicio FastAPI que actÃºa como cerebro central del sistema, orquestando conversaciones inteligentes con **Gemini** usando **RAG** (Retrieval-Augmented Generation) y **Tool Calling**.

## ï¿½ PropÃ³sito

El orquestador es el **intermediario inteligente** entre el frontend (React) y el backend de productos (FastAPI + PostgreSQL). Su funciÃ³n es:

- **Recibir** mensajes del usuario desde el frontend
- **Decidir** quÃ© informaciÃ³n necesita del backend usando IA
- **Ejecutar** llamadas a funciones automÃ¡ticamente (Tool Calling)
- **Generar** respuestas conversacionales naturales

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React + TypeScript)                â”‚
â”‚                      http://localhost:5173                      â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Interfaz de chat                                             â”‚
â”‚  â€¢ Captura mensajes del usuario                                 â”‚
â”‚  â€¢ Renderiza respuestas del bot                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ POST /api/chat
                             â”‚ { 
                             â”‚   "message": "Â¿Hay stock de S001?",
                             â”‚   "conversation_history": [...]
                             â”‚ }
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MICROSERVICIO ORQUESTADOR LLM (FastAPI) â† AQUÃ          â”‚
â”‚                   http://localhost:8001                         â”‚
â”‚                                                                 â”‚
â”‚  ğŸ§  PROCESAMIENTO INTELIGENTE:                                  â”‚
â”‚  1. RAG: Carga catÃ¡logo de productos                            â”‚
â”‚  2. System Prompt: Define comportamiento del agente             â”‚
â”‚  3. Gemini API: Procesa mensaje + decide Tool Calling           â”‚
â”‚  4. Tool Executor: Ejecuta funciones si es necesario            â”‚
â”‚  5. Response: Genera respuesta conversacional                   â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚ Tool Calling (cuando es necesario)
                             â”‚ GET /api/products/{id}/stock
                             â”‚ GET /api/products/search/query
                             â”‚ GET /api/products/{id}
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        MICROSERVICIO DE PRODUCTOS (FastAPI)                     â”‚
â”‚                   http://localhost:8000                         â”‚
â”‚                                                                 â”‚
â”‚  â€¢ GestiÃ³n de productos y stock                                 â”‚
â”‚  â€¢ Base de datos PostgreSQL                                     â”‚
â”‚  â€¢ APIs REST para consultas                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–²
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GEMINI API (Google Cloud)                    â”‚
â”‚              https://generativelanguage.googleapis.com          â”‚
â”‚                                                                 â”‚
â”‚  â€¢ Procesamiento de lenguaje natural                            â”‚
â”‚  â€¢ Tool Calling (decisiÃ³n automÃ¡tica de llamar funciones)       â”‚
â”‚  â€¢ GeneraciÃ³n de respuestas conversacionales                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ CÃ³mo Levantar el Servicio

### OpciÃ³n 1: Docker (Recomendado)

```bash
# Navegar al directorio
cd back_expo_orchestrator

# Construir imagen
docker build -t llm-orchestrator .

# Ejecutar contenedor
docker run -p 8001:8001 --env-file .env llm-orchestrator
```

### OpciÃ³n 2: Localmente

```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar servidor
python -m uvicorn app.main:app --host 0.0.0.0 --port 8001
```

## ğŸ› ï¸ Estructura del Proyecto

```
app/
â”œâ”€â”€ main.py                    # ğŸš€ FastAPI app principal + CORS
â”œâ”€â”€ config.py                  # âš™ï¸ ConfiguraciÃ³n (API keys, URLs)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ chat.py               # ğŸ“ Modelos: ChatRequest, ChatResponse
â”‚
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ chat.py               # ğŸŒ Endpoints: POST /api/chat
â”‚
â”œâ”€â”€ services/                  # ğŸ§  SERVICIOS CORE
â”‚   â”œâ”€â”€ gemini_service.py     # ğŸ¤– LÃ³gica de Gemini + RAG
â”‚   â”œâ”€â”€ tool_executor.py      # ğŸ”§ Ejecutor de Tool Calling
â”‚   â””â”€â”€ rag_service.py        # ğŸ“š Carga catÃ¡logo para contexto
â”‚
â””â”€â”€ schemas/
    â””â”€â”€ tools.py              # ğŸ› ï¸ Tool Schemas (definiciÃ³n de funciones)
```

## ğŸ¤– Funcionamiento del LLM (Gemini)

### 1. **System Prompt + RAG Context**
```
System Instruction = Base Prompt + CatÃ¡logo de Productos
```

- **Base Prompt**: Define que es un "Agente de Soporte de Pedidos"
- **RAG Context**: Inyecta lista de productos disponibles (S001: Silla $299, T010: Teclado $99...)
- **Restricciones**: Solo habla de tecnologÃ­a, nunca inventa precios/stock

### 2. **Procesamiento del Mensaje**
```
Usuario: "Â¿Hay stock del mouse M001?"
   â†“
Gemini analiza: "Necesito verificar stock en tiempo real"
   â†“
Gemini genera: functionCall("verificar_stock", {"product_id": "M001"})
```

### 3. **Respuesta Final**
```
Tool Result: {"stock": 50, "status": "available"}
   â†“
Gemini genera: "El Mouse InalÃ¡mbrico Gamer (M001) tiene 50 unidades disponibles."
```

## ï¿½ Funcionamiento del Tool Calling

### **Tool Schemas Disponibles:**

1. **`verificar_stock(product_id)`**
   - Verifica inventario en tiempo real
   - Llama: `GET /api/products/{id}/stock`

2. **`buscar_productos(query, limit=5)`**
   - Busca productos por tÃ©rmino
   - Llama: `GET /api/products/search/query`

3. **`consultar_precio(product_id)`**
   - Obtiene precio actualizado
   - Llama: `GET /api/products/{id}`

### **Flujo de Tool Calling:**

```
1. LLM decide: "Necesito llamar una funciÃ³n"
   â†“
2. Orquestador ejecuta: await tool_executor.execute()
   â†“
3. HTTP Request al backend de productos
   â†“
4. Resultado vuelve al LLM
   â†“
5. LLM puede decidir llamar OTRA funciÃ³n (recursive calling)
   â†“
6. Finalmente genera respuesta en lenguaje natural
```

## ğŸ”— RelaciÃ³n con Otros Servicios

### **Frontend â†’ Orquestador**
- **Protocolo**: HTTP REST
- **Endpoint**: `POST /api/chat`
- **Datos**: Mensaje + historial de conversaciÃ³n
- **Respuesta**: Texto + metadata (funciones llamadas, tokens usados)

### **Orquestador â†’ Backend Productos**
- **Protocolo**: HTTP REST
- **Endpoints**: `/api/products/*` (stock, search, pricing)
- **PropÃ³sito**: Tool Calling automÃ¡tico segÃºn decisiones del LLM
- **Datos**: IDs de productos, tÃ©rminos de bÃºsqueda

### **Orquestador â†’ Gemini API**
- **Protocolo**: HTTPS REST
- **Endpoint**: `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview:generateContent`
- **Datos**: System Prompt + Tool Schemas + ConversaciÃ³n
- **Respuesta**: Texto OR functionCall

## ğŸ“Š Tipos de Respuesta

### **Respuesta Directa (Sin Tool Calling)**
```json
{
  "response": "Vendemos sillas ergonÃ³micas, teclados mecÃ¡nicos, monitores...",
  "functions_called": null,
  "tokens_used": {"total_tokens": 1200}
}
```

### **Respuesta con Tool Calling**
```json
{
  "response": "El Mouse M001 tiene 50 unidades disponibles.",
  "functions_called": [
    {
      "name": "verificar_stock",
      "args": {"product_id": "M001"},
      "result": {"stock": 50, "status": "available"}
    }
  ],
  "tokens_used": {"total_tokens": 1800}
}
```

## ï¿½ TecnologÃ­as Utilizadas

### **Backend Framework**
- **FastAPI**: Web framework asÃ­ncrono para APIs REST
- **Uvicorn**: Servidor ASGI de alta performance
- **Pydantic**: ValidaciÃ³n de datos y serializaciÃ³n

### **LLM Integration**
- **Gemini 2.5 Flash Preview**: Modelo de Google para conversaciÃ³n + Tool Calling
- **httpx**: Cliente HTTP asÃ­ncrono para llamadas a APIs

### **Arquitectura**
- **RAG (Retrieval-Augmented Generation)**: Contexto de productos
- **Tool Calling**: EjecuciÃ³n automÃ¡tica de funciones
- **Microservicios**: SeparaciÃ³n de responsabilidades

### **Deployment**
- **Docker**: ContainerizaciÃ³n del servicio
- **Environment Variables**: ConfiguraciÃ³n segura (API keys)

## ï¿½ Endpoints Principales

| Endpoint | MÃ©todo | PropÃ³sito |
|----------|--------|-----------|
| `/api/chat` | POST | ConversaciÃ³n principal con el LLM |
| `/api/chat/reset` | POST | Reiniciar contexto de conversaciÃ³n |
| `/api/chat/health` | GET | Health check del servicio |
| `/health` | GET | Health check general |
| `/docs` | GET | DocumentaciÃ³n Swagger UI |

## ğŸ§ª Testing RÃ¡pido

```bash
# Health check
curl http://localhost:8001/health

# Mensaje simple
curl -X POST http://localhost:8001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Â¿QuÃ© productos tienes?"}'

# Mensaje que requiere Tool Calling
curl -X POST http://localhost:8001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Â¿Hay stock del producto S001?"}'
```
